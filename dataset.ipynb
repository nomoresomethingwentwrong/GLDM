{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a48ba5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !molecule_generation preprocess INPUT_DIR OUTPUT_DIR TRACE_DIR --generation-order=canonical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5bfe46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/data/ongh0068/l1000/moler_reference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2b6b2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-07 08:29:38.848042: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "\n",
    "with gzip.open('/data/ongh0068/l1000/trace_playground/metadata.pkl.gz', 'rb') as f:\n",
    "     metadata= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d14815a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "\n",
    "with gzip.open('/data/ongh0068/l1000/trace_playground/train_0/train_0.pkl.gz', 'rb') as f:\n",
    "     train= pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722bd97b",
   "metadata": {},
   "source": [
    "# Misc TODOs\n",
    "\n",
    "1. Add a motif embedding layer to the GNN => need to look into whether there are specific gnn layers for this\n",
    "2. Dataset => validate if it is correct\n",
    "3. Implement function for storing Data objects in the Pytorch geometric dataset\n",
    "4. Implement all the helper function for the pytorch geometric dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3b2f61",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "The original dataset uses a function to randomly decide whether to include or exclude a particular partial graph from the batch. Instead of that, we try to simply save all Data objects a separate .pt files => at training time, we list all the file paths and then sample from the file paths uniformly \n",
    "\n",
    "- Downsides => will occupy more memory: potential optimisations (TODOs) would be to save as compressed file and only decompress at training time.\n",
    "- Upside => less preprocessing time at training time\n",
    "\n",
    "\n",
    "There will be 1 dataset object for each of the train/validation/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e593694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2de30442",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for step in train[0]:\n",
    "#     print(step)\n",
    "    count += 1\n",
    "    if count == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18086a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Dataset\n",
    "import os\n",
    "from molecule_generation.chem.motif_utils import get_motif_type_to_node_type_index_map\n",
    "\n",
    "\n",
    "def get_motif_type_to_node_type_index_map(\n",
    "    motif_vocabulary, num_atom_types\n",
    "):\n",
    "    \"\"\"Helper to construct a mapping from motif type to shifted node type.\"\"\"\n",
    "\n",
    "    return {\n",
    "        motif: num_atom_types + motif_type\n",
    "        for motif, motif_type in motif_vocabulary.vocabulary.items()\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "class MolerDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        root, \n",
    "        raw_moler_trace_dataset_parent_folder, # absolute path \n",
    "        output_pyg_trace_dataset_parent_folder, # absolute path\n",
    "        split = 'train',\n",
    "        transform=None, \n",
    "        pre_transform=None, \n",
    "    ):\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self._transform = transform \n",
    "        self._pre_transform = pre_transform\n",
    "        self._raw_moler_trace_dataset_parent_folder = raw_moler_trace_dataset_parent_folder\n",
    "        self._output_pyg_trace_dataset_parent_folder = output_pyg_trace_dataset_parent_folder\n",
    "        self._split = split\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \"\"\"\n",
    "        Raw generation trace files output from the preprocess function of the cli. These are zipped pickle\n",
    "        files. This is the actual file name without the parent folder.\n",
    "        \"\"\"\n",
    "        raw_file_paths_folder = os.path.join(self._raw_moler_trace_dataset_parent_folder, self._split)\n",
    "        assert os.path.exist(raw_file_paths_folder), f'{raw_file_paths_folder} does not exist.'\n",
    "        raw_generation_trace_files = [file_path for file_path in os.listdir(raw_file_paths_folder)]\n",
    "        return raw_generation_trace_files\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\"Processed generation trace objects that are stored as .pt files\"\"\"\n",
    "        processed_file_paths_folder = os.path.join(self._output_pyg_trace_dataset_parent_folder, self._split)\n",
    "        assert os.path.exist(raw_file_paths_folder), f'{raw_file_paths_folder} does not exist.'\n",
    "        processed_generation_trace_files = [os.path.join(file)  for file in os.listdir(processed_file_paths_folder)]\n",
    "        return processed_generation_trace_files\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names_size(self):\n",
    "        return len(self.processed_file_names)\n",
    "    \n",
    "    @property \n",
    "    def metadata(self):\n",
    "        return self._metadata\n",
    "\n",
    "    def node_types_to_multi_hot(self, node_types):\n",
    "        \"\"\"Convert between string representation to multi hot encoding of correct node types.\n",
    "\n",
    "        Note: implemented here for backwards compatibility only.\n",
    "        \"\"\"\n",
    "        return np.zeros(shape=(self.num_node_types,), dtype=np.float32)\n",
    "\n",
    "    def node_type_to_index(self, node_type):\n",
    "        motif_node_type_index = self._motif_to_node_type_index.get(node_type)\n",
    "\n",
    "        if motif_node_type_index is not None:\n",
    "            return motif_node_type_index\n",
    "        else:\n",
    "            return self._atom_type_featuriser.type_name_to_index(node_type)\n",
    "    \n",
    "    @property\n",
    "    def num_node_types(self):\n",
    "        return len(self.node_type_index_to_string)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def load_metadata(self, metadata_path):\n",
    "        metadata_file_path = os.path.join(self._raw_moler_trace_dataset_parent_folder, 'metadata.pkl.gz')\n",
    "        \n",
    "        with gzip.open(metadata_file_path, 'rb') as f:\n",
    "             self._metadata = pickle.load(f)\n",
    "        \n",
    "        self._atom_type_featuriser = next(\n",
    "            featuriser\n",
    "            for featuriser in self._metadata[\"feature_extractors\"]\n",
    "            if featuriser.name == \"AtomType\"\n",
    "        )\n",
    "        \n",
    "        self._node_type_index_to_string = self._atom_type_featuriser.index_to_atom_type_map.copy()\n",
    "        self._motif_vocabulary = self.metadata.get(\"motif_vocabulary\")\n",
    "\n",
    "        if self._motif_vocabulary is not None:\n",
    "            self._motif_to_node_type_index = get_motif_type_to_node_type_index_map(\n",
    "                motif_vocabulary=self._motif_vocabulary,\n",
    "                num_atom_types=len(self._node_type_index_to_string),\n",
    "            )\n",
    "\n",
    "            for motif, node_type in self._motif_to_node_type_index.items():\n",
    "                self._node_type_index_to_string[node_type] = motif\n",
    "        else:\n",
    "            self._motif_to_node_type_index = {}\n",
    "        \n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"Convert raw generation traces into individual .pt files for each of the trace steps.\"\"\"\n",
    "        # only call process if it was not called before\n",
    "        if self.processed_file_names_size > 0:\n",
    "            pass\n",
    "        else:\n",
    "            for pkl_file_path in self.raw_file_names:\n",
    "                generation_steps = self._convert_data_shard_to_list_of_trace_steps(pkl_file_path)\n",
    "                for molecule_idx, molecule_gen_steps in generation_steps:\n",
    "                    for step_idx, step in enumerate(molecule_gen_steps):\n",
    "                        file_name = f'{pkl_file_path}_mol_{molecule_idx}_step_{step_idx}.pt'\n",
    "                        file_path = os.path.join(self._output_pyg_trace_dataset_parent_folder, file_name)\n",
    "                        torch.save(step, file_path)\n",
    "\n",
    "    def _convert_data_shard_to_list_of_trace_steps(self, pkl_file_path):\n",
    "        # TODO: multiprocessing to speed this up\n",
    "        generation_steps = []\n",
    "        \n",
    "        with gzip.open(pkl_file_path, 'rb') as f:\n",
    "            molecules = pickle.load(f)\n",
    "            for molecule_idx, molecule in enumerate(molecules): \n",
    "                generation_steps.extend([i, self._extract_generation_steps(molecule)])\n",
    "                \n",
    "        return generation_steps\n",
    "            \n",
    "    def _extract_generation_steps(self, molecule):\n",
    "        for gen_step in molecule:\n",
    "            x = gen_step.partial_node_features\n",
    "            node_categorical_features = gen_step.node_categorical_features\n",
    "        \n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31853d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "cbdcd0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('/data/ongh0068/l1000/trace_playground/metadata.pkl.gz', 'rb') as f:\n",
    "     metadata= pickle.load(f)\n",
    "\n",
    "atom_type_featuriser = next(\n",
    "    featuriser\n",
    "    for featuriser in metadata[\"feature_extractors\"]\n",
    "    if featuriser.name == \"AtomType\"\n",
    ")\n",
    "\n",
    "node_type_index_to_string = atom_type_featuriser.index_to_atom_type_map.copy()\n",
    "motif_vocabulary = metadata.get(\"motif_vocabulary\")\n",
    "\n",
    "if motif_vocabulary is not None:\n",
    "    motif_to_node_type_index = get_motif_type_to_node_type_index_map(\n",
    "        motif_vocabulary=motif_vocabulary,\n",
    "        num_atom_types=len(node_type_index_to_string),\n",
    "    )\n",
    "\n",
    "    for motif, node_type in motif_to_node_type_index.items():\n",
    "        node_type_index_to_string[node_type] = motif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "8564bb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_997897/2386230476.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  molecule_gen_steps[i][k] = torch.tensor(molecule_gen_steps[i][k])\n"
     ]
    }
   ],
   "source": [
    "def num_node_types():\n",
    "    return len(node_type_index_to_string)\n",
    "\n",
    "def node_type_to_index(node_type):\n",
    "    return atom_type_featuriser.type_name_to_index(node_type)\n",
    "\n",
    "def node_types_to_indices(node_types):\n",
    "    node_type_to_index\n",
    "    \"\"\"Convert list of string representations into list of integer indices.\"\"\"\n",
    "    return [node_type_to_index(node_type) for node_type in node_types]\n",
    "\n",
    "def node_types_to_multi_hot(node_types):\n",
    "    \"\"\"Convert between string representation to multi hot encoding of correct node types.\n",
    "\n",
    "    Note: implemented here for backwards compatibility only.\n",
    "    \"\"\"\n",
    "    num_node_types\n",
    "    correct_indices = node_types_to_indices(node_types)\n",
    "    multihot = np.zeros(shape=(num_node_types(),), dtype=np.float32)\n",
    "    for idx in correct_indices:\n",
    "        multihot[idx] = 1.0\n",
    "    return multihot\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "train_sample = train[0]\n",
    "\n",
    "\n",
    "graph_property_values = dict(train_sample.graph_property_values)\n",
    "\n",
    "molecule_gen_steps = []\n",
    "for gen_step in train_sample:\n",
    "    gen_step_features = {}\n",
    "    gen_step_features['x'] = gen_step.partial_node_features\n",
    "    gen_step_features['focus_node'] = gen_step.focus_node\n",
    "    \n",
    "    # have an edge type attribute to tell apart each of the 3 bond types\n",
    "    edge_indexes = []\n",
    "    edge_types= []\n",
    "    for i, adj_list in enumerate(gen_step.partial_adjacency_lists):\n",
    "        if len(adj_list) != 0:\n",
    "            edge_index = torch.tensor(adj_list).T\n",
    "            edge_indexes += [edge_index]\n",
    "            edge_types += [i]*len(adj_list)\n",
    "    gen_step_features['edge_index'] = torch.cat(edge_indexes, 1)\n",
    "    gen_step_features['edge_type'] = torch.tensor(edge_types)\n",
    "    gen_step_features['correct_edge_choices'] = gen_step.correct_edge_choices\n",
    "    \n",
    "    # TODO for correct edge choices, since some steps have, some steps don't\n",
    "    # we need to subclass the dataloader's collate function and add a \n",
    "    # attribute similar to ptr to get the idx of the graphs that require this\n",
    "    \n",
    "    # For all other attributes that have always have the same dimensions\n",
    "    # but might be empty, we can use -1 as a placeholder\n",
    "    \n",
    "    \n",
    "    num_correct_edge_choices = np.sum(gen_step.correct_edge_choices)\n",
    "    gen_step_features['num_correct_edge_choices'] = num_correct_edge_choices\n",
    "    gen_step_features['stop_node_label'] = int(num_correct_edge_choices == 0)\n",
    "    gen_step_features['valid_edge_choices'] = gen_step.valid_edge_choices\n",
    "    gen_step_features[\"correct_edge_types\"] = gen_step.correct_edge_types\n",
    "    gen_step_features[\"partial_node_categorical_features\"] = gen_step.partial_node_categorical_features\n",
    "    if gen_step.correct_attachment_point_choice is not None:\n",
    "        gen_step_features[\"correct_attachment_point_choice\"] = list(gen_step.valid_attachment_point_choices).index(gen_step.correct_attachment_point_choice)\n",
    "    else:\n",
    "        gen_step_features[\"correct_attachment_point_choice\"] = []\n",
    "    gen_step_features[\"valid_attachment_point_choices\"] = gen_step.valid_attachment_point_choices\n",
    "    \n",
    "    # And finally, the correct node type choices. Here, we have an empty list of\n",
    "    # correct choices for all steps where we didn't choose a node, so we skip that:\n",
    "    if gen_step.correct_node_type_choices is not None:\n",
    "        gen_step_features[\"correct_node_type_choices\"] = node_types_to_multi_hot(gen_step.correct_node_type_choices)\n",
    "    else:\n",
    "        gen_step_features[\"correct_node_type_choices\"] = []\n",
    "    gen_step_features['correct_first_node_type_choices'] = node_types_to_multi_hot(train_sample.correct_first_node_type_choices)\n",
    "    \n",
    "    # Add graph_property_values\n",
    "    gen_step_features = {**gen_step_features, **graph_property_values}\n",
    "    \n",
    "    molecule_gen_steps += [gen_step_features]\n",
    "for i in range(len(molecule_gen_steps)):\n",
    "    for k,v in molecule_gen_steps[i].items():\n",
    "        molecule_gen_steps[i][k] = torch.tensor(molecule_gen_steps[i][k])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "79e28771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "# class MolerTraceData(Data):\n",
    "#     def __cat_dim__(self, key, value, *args, **kwargs):\n",
    "#         if key == 'foo':\n",
    "#             return None\n",
    "#     else:\n",
    "#         return super().__cat_dim__(key, value, *args, **kwargs)\n",
    "\n",
    "\n",
    "pyg_data = []\n",
    "for step in molecule_gen_steps:\n",
    "    pyg_data.append(Data(**step))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "24b7beba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "loader = DataLoader(pyg_data, batch_size=16, shuffle=False, follow_batch = [\n",
    "    'correct_edge_choices',\n",
    "    'correct_edge_types',\n",
    "    'valid_edge_choices',\n",
    "    'valid_attachment_point_choices',\n",
    "    'correct_attachment_point_choice',\n",
    "    'correct_node_type_choices'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "f37e1016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  0,  2,  3,  5,  6,  9, 11, 15, 18, 23, 27, 33, 38, 42, 48, 53]),\n",
       " tensor([   0,  139,  139,  278,  278,  417,  417,  556,  556,  695,  695,  834,\n",
       "          834,  834,  973,  973, 1112]))"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.correct_edge_choices_ptr, batch.correct_node_type_choices_ptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "6e48cf8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  1,  2,  3,  3,  4,  5,  5,  5,  6,  6,  7,  7,  7,  7,  8,  8,  8,\n",
       "         9,  9,  9,  9,  9, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 12, 12, 12,\n",
       "        12, 12, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.correct_edge_choices_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "be795e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([53]), torch.Size([1112]))"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.correct_edge_choices.shape, batch.correct_node_type_choices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "c096cab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([126, 32])\n",
      "edge_index: torch.Size([2, 212])\n",
      "focus_node: torch.Size([16])\n",
      "edge_type: torch.Size([212])\n",
      "correct_edge_choices: torch.Size([53])\n",
      "correct_edge_choices_batch: torch.Size([53])\n",
      "correct_edge_choices_ptr: torch.Size([17])\n",
      "num_correct_edge_choices: torch.Size([16])\n",
      "stop_node_label: torch.Size([16])\n",
      "valid_edge_choices: torch.Size([53, 2])\n",
      "valid_edge_choices_batch: torch.Size([53])\n",
      "valid_edge_choices_ptr: torch.Size([17])\n",
      "correct_edge_types: torch.Size([9, 3])\n",
      "correct_edge_types_batch: torch.Size([9])\n",
      "correct_edge_types_ptr: torch.Size([17])\n",
      "partial_node_categorical_features: torch.Size([126])\n",
      "correct_attachment_point_choice: torch.Size([0])\n",
      "correct_attachment_point_choice_batch: torch.Size([0])\n",
      "correct_attachment_point_choice_ptr: torch.Size([17])\n",
      "valid_attachment_point_choices: torch.Size([0])\n",
      "valid_attachment_point_choices_batch: torch.Size([0])\n",
      "valid_attachment_point_choices_ptr: torch.Size([17])\n",
      "correct_node_type_choices: torch.Size([1112])\n",
      "correct_node_type_choices_batch: torch.Size([1112])\n",
      "correct_node_type_choices_ptr: torch.Size([17])\n",
      "correct_first_node_type_choices: torch.Size([2224])\n",
      "sa_score: torch.Size([16])\n",
      "clogp: torch.Size([16])\n",
      "mol_weight: torch.Size([16])\n",
      "qed: torch.Size([16])\n",
      "bertz: torch.Size([16])\n",
      "batch: torch.Size([126])\n",
      "ptr: torch.Size([17])\n"
     ]
    }
   ],
   "source": [
    "def pprint_pyg_obj(batch):\n",
    "    for key in vars(batch)['_store'].keys():\n",
    "        if key.startswith('_'):\n",
    "            continue\n",
    "        print(f'{key}: {batch[key].shape}')\n",
    "for batch in loader:\n",
    "    pprint_pyg_obj(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "9fda7deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import RGATConv\n",
    "\n",
    "conv1 = RGATConv(in_channels = batch.x.shape[1], out_channels = batch.x.shape[1]//2, num_relations = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "44be02f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([126, 16])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1(batch.x, batch.edge_index.long(), batch.edge_type).shape # shape: [num_nodes, latent_feature_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "5679b3b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([ 0,  0,  0,  ..., 15, 15, 15]))"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.correct_node_type_choices, batch.correct_node_type_choices_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e9055707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.correct_edge_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3fae0dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0],\n",
       "        [1, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyg_data[11].correct_edge_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6a7c29da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   4,   9,  14,  20,  26,  33,  40,  48,  56,  65,  74,  84,  94,\n",
       "        104, 115, 126])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.ptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "982ce51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  3,  5,  7,  9, 11, 11, 12, 14])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.correct_edge_types_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "68e65f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [1, 1],\n",
       "       [2, 2],\n",
       "       [3, 3],\n",
       "       [4, 4]], dtype=int32)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(np.arange(5, dtype=np.int32), 2).reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ce4f6c",
   "metadata": {},
   "source": [
    "# Issues to resolve\n",
    "\n",
    "Here, for a batch size of 16, there are 4 nodes in the first sample in the batch and 126 nodes in total as seen from x. the `ptr` attribute tells us where each new graph starts in the batch.\n",
    "\n",
    "\n",
    "Add the `is_conjugated` and the other bond properties to the preprocessing script as used in ogb https://github.com/snap-stanford/ogb/blob/master/ogb/utils/features.py\n",
    "\n",
    "\n",
    "(DONE) Add graph property values\n",
    "\n",
    "\n",
    "Add self loops\n",
    "\n",
    "\n",
    "(DONE) There is something wrong with the multihot encoding: none of the node types are of value 1\n",
    "\n",
    "(DONE: `follow_batch` was used instead of the things below)\n",
    "1. edge_index: how do we tell which belongs to which graph? Does it matter?\n",
    "2. focus node: NI\n",
    "3. edge type: same as edge index, how to tell?\n",
    "4. correct_edge_choices: need to put a -1 for those samples without any?\n",
    "5. num_correct_edge_choices: NI, for each of the 16 samples => can be used to infer which graphs to use for training the decoder mlps\n",
    "6. stop_node_label: NI\n",
    "7. valid_edge_choices: need to put -1 for samples without any\n",
    "8. correct_edge_types: need to put -1 for samples without any \n",
    "9. partial_node_categorical_features: NI\n",
    "10. correct_attachment_point_choice: need to put -1 for samples without any\n",
    "11. valid_attachment_point_choices: need to put -1 for samples without any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "01024736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.correct_node_type_choices.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "95d48eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 56,  65,  74,  84,  94, 104, 115, 126])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.ptr[batch.ptr > 50] # use this kind of numpy like indexing to get \n",
    "# the values we want\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "875289e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C1=CC=CC=C1': 0,\n",
       " 'C1CCNCC1': 1,\n",
       " 'C1=CC=NC=C1': 2,\n",
       " 'NC=O': 3,\n",
       " 'C1CNCCN1': 4,\n",
       " 'C1CCNC1': 5,\n",
       " 'O=[N+][O-]': 6,\n",
       " 'C1CC1': 7,\n",
       " 'C1COCCN1': 8,\n",
       " 'O=CO': 9,\n",
       " 'N=CO': 10,\n",
       " 'C1=CNN=C1': 11,\n",
       " 'FC(F)F': 12,\n",
       " 'CC=O': 13,\n",
       " 'C1CCOCC1': 14,\n",
       " 'C1=CC=C2C=CC=CC2=C1': 15,\n",
       " 'COC=O': 16,\n",
       " 'CCO': 17,\n",
       " 'C=CC': 18,\n",
       " 'C1=CSC=N1': 19,\n",
       " 'C1=CSC=C1': 20,\n",
       " 'C1=CON=C1': 21,\n",
       " 'C1=CC=C2NC=CC2=C1': 22,\n",
       " 'OC(F)F': 23,\n",
       " 'O=CNO': 24,\n",
       " 'CNC=O': 25,\n",
       " 'CCNC(=O)OC': 26,\n",
       " 'CCCO': 27,\n",
       " 'CCC(N)=O': 28,\n",
       " 'CCC(=O)O': 29,\n",
       " 'CC(N)=O': 30,\n",
       " 'CC(C)C': 31,\n",
       " 'C1CCOC1': 32,\n",
       " 'C1CCCCC1': 33,\n",
       " 'C1=NC=C2CCCCC2=N1': 34,\n",
       " 'C1=COC=C1': 35,\n",
       " 'C1=CNC=C1': 36,\n",
       " 'C1=CN=CN=C1': 37,\n",
       " 'C1=CN=C2C=CC=CC2=C1': 38,\n",
       " 'C1=CN2N=CC=C2N=C1': 39,\n",
       " 'C1=CC=C2OC=CCC2=C1': 40,\n",
       " 'C1=CC=C2N=CC=CC2=C1': 41,\n",
       " 'C1=CC2=C(CCC2)S1': 42,\n",
       " 'O=S=O': 43,\n",
       " 'O=CNCCO': 44,\n",
       " 'O=CNCCCCCNC=O': 45,\n",
       " 'O=CNCCC(=O)O': 46,\n",
       " 'O=CCCO': 47,\n",
       " 'O=C(O)CS': 48,\n",
       " 'N[SH](=O)=O': 49,\n",
       " 'NNC=O': 50,\n",
       " 'NCCO': 51,\n",
       " 'NCCCC(N)=O': 52,\n",
       " 'NC(N)=O': 53,\n",
       " 'ClC(Cl)Cl': 54,\n",
       " 'CSCCC(N)C(=O)O': 55,\n",
       " 'COS(=O)(=O)CCC(N)=O': 56,\n",
       " 'CNCCOC': 57,\n",
       " 'CNC(C)=O': 58,\n",
       " 'CNC(=O)CS': 59,\n",
       " 'CNC(=O)C(CC(=O)NC(CC(C)C)B(O)O)NC(=O)OC(C)(C)C': 60,\n",
       " 'CNC': 61,\n",
       " 'CN=CO': 62,\n",
       " 'CN(C)C(N)=O': 63,\n",
       " 'CCOCC': 64,\n",
       " 'CCOC=O': 65,\n",
       " 'CCOC': 66,\n",
       " 'CCNC(C)=O': 67,\n",
       " 'CCNC(=O)CC': 68,\n",
       " 'CCN(C)C': 69,\n",
       " 'CCN': 70,\n",
       " 'CCCNC(=O)NCCCCCCCCCCCCCCCC(=O)O': 71,\n",
       " 'CCCNC(=O)CCCNC=O': 72,\n",
       " 'CCCCNCC': 73,\n",
       " 'CCCCCO': 74,\n",
       " 'CCCCCCCO': 75,\n",
       " 'CCCCCCC=CC(C)=O': 76,\n",
       " 'CCCCCC(C)C': 77,\n",
       " 'CCCCCC(=O)OC': 78,\n",
       " 'CCCCC': 79,\n",
       " 'CCCC(C)N': 80,\n",
       " 'CCCC(=O)NCCC#N': 81,\n",
       " 'CCCC': 82,\n",
       " 'CCC(COC(=O)C(C)(C)C)NC(=S)NC': 83,\n",
       " 'CCC(C)C(NC=O)C(N)=O': 84,\n",
       " 'CCC(C)C(NC(=O)C(N)CO)C(=O)NC(C)C(N)=O': 85,\n",
       " 'CCC': 86,\n",
       " 'CC=NNC=S': 87,\n",
       " 'CC(NC=O)C(N)=O': 88,\n",
       " 'CC(NC=O)C(=O)CCC=O': 89,\n",
       " 'CC(N)C(=O)NC(C=O)C(C)C': 90,\n",
       " 'CC(C)SCC(O)C(C)NC(=O)C(C)NC(=O)C(C)NC(=O)OC(C)(C)C': 91,\n",
       " 'CC(C)O': 92,\n",
       " 'CC(C)C(N)=O': 93,\n",
       " 'CC(=O)OCC(N)=O': 94,\n",
       " 'CC(=O)O': 95,\n",
       " 'CC(=O)NCC(=O)O': 96,\n",
       " 'CC(=O)N(C)C': 97,\n",
       " 'CC#N': 98,\n",
       " 'C=NO': 99,\n",
       " 'C=NNC=O': 100,\n",
       " 'C=NNC(=O)NCCCC': 101,\n",
       " 'C=CCOC(C)=O': 102,\n",
       " 'C=CC(=O)C=C(O)C=C': 103,\n",
       " 'C1OC2CNC1C2': 104,\n",
       " 'C1NCC12COC2': 105,\n",
       " 'C1NC2C3C4CC5C3C1C1C5C4C21': 106,\n",
       " 'C1CNCNC1': 107,\n",
       " 'C1CNCCNC1': 108,\n",
       " 'C1CNC1': 109,\n",
       " 'C1CCC2CCCC2C1': 110,\n",
       " 'C1CCC2C(C1)CCC1C3CCCC3CCC21': 111,\n",
       " 'C1CCC2=C(C1)CC1=C(CCCC1)O2': 112,\n",
       " 'C1CC2OCCC2CN1': 113,\n",
       " 'C1CC2CCC1C2': 114,\n",
       " 'C1C2CC3CC1CC(C2)C3': 115,\n",
       " 'C1=NO[N+]=C1': 116,\n",
       " 'C1=NNN=N1': 117,\n",
       " 'C1=NNC2=C1CCCC2': 118,\n",
       " 'C1=NN=NN1': 119,\n",
       " 'C1=NN=C2SC=NN12': 120,\n",
       " 'C1=NN2CCCNC2=C1': 121,\n",
       " 'C1=NC=NO1': 122,\n",
       " 'C1=NC=NN1': 123,\n",
       " 'C1=NC=NC=N1': 124,\n",
       " 'C1=NC=NC2=C1N=CN2': 125,\n",
       " 'C1=NC2=C(CNCN2)N1': 126,\n",
       " 'C1=CSCN1': 127}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata['motif_vocabulary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ab9a770f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COC=O']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample.correct_first_node_type_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c0dedcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<rdkit.Chem.rdchem.Mol at 0x7fb98d077b30>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "Chem.MolFromSmiles('COC=O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18918b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
