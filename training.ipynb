{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e744607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/data/ongh0068/l1000/pyg_output_playground/train/processed_file_paths.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a8a087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/data/ongh0068/l1000/moler_reference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac216788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 07:18:17.097596: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Dataset, Data\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from molecule_generation.chem.motif_utils import get_motif_type_to_node_type_index_map\n",
    "import torch\n",
    "import gzip\n",
    "import pickle\n",
    "def get_motif_type_to_node_type_index_map(\n",
    "    motif_vocabulary, num_atom_types\n",
    "):\n",
    "    \"\"\"Helper to construct a mapping from motif type to shifted node type.\"\"\"\n",
    "\n",
    "    return {\n",
    "        motif: num_atom_types + motif_type\n",
    "        for motif, motif_type in motif_vocabulary.vocabulary.items()\n",
    "    }\n",
    "\n",
    "\n",
    "class MolerDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        root, \n",
    "        raw_moler_trace_dataset_parent_folder, # absolute path \n",
    "        output_pyg_trace_dataset_parent_folder, # absolute path\n",
    "        split = 'train',\n",
    "        transform=None, \n",
    "        pre_transform=None, \n",
    "    ):\n",
    "        self._processed_file_paths = None\n",
    "        self._transform = transform \n",
    "        self._pre_transform = pre_transform\n",
    "        self._raw_moler_trace_dataset_parent_folder = raw_moler_trace_dataset_parent_folder\n",
    "        self._output_pyg_trace_dataset_parent_folder = output_pyg_trace_dataset_parent_folder\n",
    "        self._split = split\n",
    "        self.load_metadata()\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \"\"\"\n",
    "        Raw generation trace files output from the preprocess function of the cli. These are zipped pickle\n",
    "        files. This is the actual file name without the parent folder.\n",
    "        \"\"\"\n",
    "        raw_pkl_file_folders = [folder for folder in os.listdir(self._raw_moler_trace_dataset_parent_folder) if folder.startswith(self._split)]\n",
    "\n",
    "        assert len(raw_pkl_file_folders) > 0, f'{self._raw_moler_trace_dataset_parent_folder} does not contain {self._split} files.'\n",
    "        \n",
    "        raw_generation_trace_files = []\n",
    "        for folder in raw_pkl_file_folders:\n",
    "            for pkl_file in os.listdir(os.path.join(self._raw_moler_trace_dataset_parent_folder, folder)):\n",
    "                raw_generation_trace_files.append(os.path.join(self._raw_moler_trace_dataset_parent_folder, folder, pkl_file))\n",
    "        return raw_generation_trace_files\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\"Processed generation trace objects that are stored as .pt files\"\"\"\n",
    "        processed_file_paths_folder = os.path.join(self._output_pyg_trace_dataset_parent_folder, self._split)\n",
    "        if self._processed_file_paths is not None:\n",
    "            return self._processed_file_paths\n",
    "        \n",
    "        if not os.path.exists(processed_file_paths_folder):\n",
    "            os.mkdir(processed_file_paths_folder)\n",
    "            return []\n",
    "        else:\n",
    "            processed_file_paths_csv = os.path.join(processed_file_paths_folder, 'processed_file_paths.csv')\n",
    "            if not os.path.exists(processed_file_paths_csv):\n",
    "                return []\n",
    "            else:\n",
    "                self._processed_file_paths = pd.read_csv(processed_file_paths_csv)['file_names'].tolist()\n",
    "                return self._processed_file_paths\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names_size(self):\n",
    "        return len(self.processed_file_names)\n",
    "    \n",
    "    @property \n",
    "    def metadata(self):\n",
    "        return self._metadata\n",
    "\n",
    "    @property\n",
    "    def node_type_index_to_string(self):\n",
    "        return self._node_type_index_to_string\n",
    "    \n",
    "    @property \n",
    "    def num_node_types(self):\n",
    "        return len(self.node_type_index_to_string)\n",
    "\n",
    "    def node_type_to_index(self, node_type):\n",
    "        return self._atom_type_featuriser.type_name_to_index(node_type)\n",
    "\n",
    "    def node_types_to_indices(self, node_types):\n",
    "        \"\"\"Convert list of string representations into list of integer indices.\"\"\"\n",
    "        return [self.node_type_to_index(node_type) for node_type in node_types]\n",
    "\n",
    "    def node_types_to_multi_hot(self, node_types):\n",
    "        \"\"\"Convert between string representation to multi hot encoding of correct node types.\n",
    "\n",
    "        Note: implemented here for backwards compatibility only.\n",
    "        \"\"\"\n",
    "        correct_indices = self.node_types_to_indices(node_types)\n",
    "        multihot = np.zeros(shape=(self.num_node_types,), dtype=np.float32)\n",
    "        for idx in correct_indices:\n",
    "            multihot[idx] = 1.0\n",
    "        return multihot\n",
    "    \n",
    "    def node_type_to_index(self, node_type):\n",
    "        motif_node_type_index = self._motif_to_node_type_index.get(node_type)\n",
    "\n",
    "        if motif_node_type_index is not None:\n",
    "            return motif_node_type_index\n",
    "        else:\n",
    "            return self._atom_type_featuriser.type_name_to_index(node_type)\n",
    "    \n",
    "    def load_metadata(self):\n",
    "        metadata_file_path = os.path.join(self._raw_moler_trace_dataset_parent_folder, 'metadata.pkl.gz')\n",
    "        \n",
    "        with gzip.open(metadata_file_path, 'rb') as f:\n",
    "             self._metadata = pickle.load(f)\n",
    "        \n",
    "        self._atom_type_featuriser = next(\n",
    "            featuriser\n",
    "            for featuriser in self._metadata[\"feature_extractors\"]\n",
    "            if featuriser.name == \"AtomType\"\n",
    "        )\n",
    "        \n",
    "        self._node_type_index_to_string = self._atom_type_featuriser.index_to_atom_type_map.copy()\n",
    "        self._motif_vocabulary = self.metadata.get(\"motif_vocabulary\")\n",
    "\n",
    "        if self._motif_vocabulary is not None:\n",
    "            self._motif_to_node_type_index = get_motif_type_to_node_type_index_map(\n",
    "                motif_vocabulary=self._motif_vocabulary,\n",
    "                num_atom_types=len(self._node_type_index_to_string),\n",
    "            )\n",
    "\n",
    "            for motif, node_type in self._motif_to_node_type_index.items():\n",
    "                self._node_type_index_to_string[node_type] = motif\n",
    "        else:\n",
    "            self._motif_to_node_type_index = {}\n",
    "\n",
    "    def generate_preprocessed_file_paths_csv(self, preprocessed_file_paths_folder):\n",
    "        file_paths = [os.path.join(preprocessed_file_paths_folder, file_path) for file_path in os.listdir(preprocessed_file_paths_folder)]\n",
    "        df = pd.DataFrame(file_paths, columns = ['file_names'])\n",
    "        processed_file_paths_csv = os.path.join(preprocessed_file_paths_folder, 'processed_file_paths.csv')\n",
    "        df.to_csv(processed_file_paths_csv, index = False)\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"Convert raw generation traces into individual .pt files for each of the trace steps.\"\"\"\n",
    "        # only call process if it was not called before\n",
    "        if self.processed_file_names_size > 0:\n",
    "            pass\n",
    "        else:\n",
    "            self.load_metadata()\n",
    "            for pkl_file_path in self.raw_file_names:\n",
    "                generation_steps = self._convert_data_shard_to_list_of_trace_steps(pkl_file_path)\n",
    "                \n",
    "                for molecule_idx, molecule_gen_steps in generation_steps:\n",
    "                    \n",
    "                    for step_idx, step in enumerate(molecule_gen_steps):\n",
    "                        file_name = f'{pkl_file_path.split(\"/\")[-1].split(\".\")[0]}_mol_{molecule_idx}_step_{step_idx}.pt'\n",
    "                        file_path = os.path.join(self._output_pyg_trace_dataset_parent_folder, self._split, file_name)\n",
    "                        print('file_path', file_path)\n",
    "                        torch.save(step, file_path)\n",
    "                        print(f'Processing {molecule_idx}, step {step_idx}')\n",
    "                        \n",
    "            self.generate_preprocessed_file_paths_csv(os.path.join(self._output_pyg_trace_dataset_parent_folder, self._split))\n",
    "\n",
    "    def _convert_data_shard_to_list_of_trace_steps(self, pkl_file_path):\n",
    "        # TODO: multiprocessing to speed this up\n",
    "        generation_steps = []\n",
    "        \n",
    "        with gzip.open(pkl_file_path, 'rb') as f:\n",
    "            molecules = pickle.load(f)\n",
    "            for molecule_idx, molecule in enumerate(molecules): \n",
    "                generation_steps += [(molecule_idx, self._extract_generation_steps(molecule))]\n",
    "        \n",
    "        return generation_steps\n",
    "            \n",
    "    def _extract_generation_steps(self, molecule):\n",
    "        molecule_gen_steps = []\n",
    "        molecule_property_values = dict(molecule.graph_property_values)\n",
    "        for gen_step in molecule:\n",
    "            gen_step_features = {}\n",
    "            \n",
    "            gen_step_features['original_graph_x'] = molecule.node_features\n",
    "            # have an edge type attribute to tell apart each of the 3 bond types\n",
    "            edge_indexes = []\n",
    "            edge_types= []\n",
    "            for i, adj_list in enumerate(molecule.adjacency_lists):\n",
    "                if len(adj_list) != 0:\n",
    "                    edge_index = adj_list.T\n",
    "                    edge_indexes += [edge_index]\n",
    "                    edge_types += [i]*len(adj_list)\n",
    "\n",
    "            \n",
    "            gen_step_features['original_graph_edge_index'] =  np.concatenate(edge_indexes, 1) if len(edge_indexes) > 0 else np.array(edge_indexes)\n",
    "            gen_step_features['original_graph_edge_type'] = np.array(edge_types)\n",
    "            gen_step_features['original_graph_node_categorical_features'] = molecule.node_categorical_features\n",
    "            \n",
    "            \n",
    "            gen_step_features['x'] = gen_step.partial_node_features\n",
    "            gen_step_features['focus_node'] = gen_step.focus_node\n",
    "\n",
    "            # have an edge type attribute to tell apart each of the 3 bond types\n",
    "            edge_indexes = []\n",
    "            edge_types= []\n",
    "            for i, adj_list in enumerate(gen_step.partial_adjacency_lists):\n",
    "                if len(adj_list) != 0:\n",
    "                    edge_index = adj_list.T\n",
    "                    edge_indexes += [edge_index]\n",
    "                    edge_types += [i]*len(adj_list)\n",
    "\n",
    "            gen_step_features['edge_index'] = np.concatenate(edge_indexes, 1) if len(edge_indexes) > 0 else np.array(edge_indexes)\n",
    "            gen_step_features['edge_type'] = np.array(edge_types)\n",
    "            gen_step_features['edge_features'] = np.array(gen_step.edge_features)\n",
    "            gen_step_features['correct_edge_choices'] = gen_step.correct_edge_choices\n",
    "            \n",
    "            num_correct_edge_choices = np.sum(gen_step.correct_edge_choices)\n",
    "            gen_step_features['num_correct_edge_choices'] = num_correct_edge_choices\n",
    "            gen_step_features['stop_node_label'] = int(num_correct_edge_choices == 0)\n",
    "            gen_step_features['valid_edge_choices'] = gen_step.valid_edge_choices\n",
    "            gen_step_features[\"correct_edge_types\"] = gen_step.correct_edge_types\n",
    "            gen_step_features[\"partial_node_categorical_features\"] = gen_step.partial_node_categorical_features\n",
    "            if gen_step.correct_attachment_point_choice is not None:\n",
    "                gen_step_features[\"correct_attachment_point_choice\"] = list(gen_step.valid_attachment_point_choices).index(gen_step.correct_attachment_point_choice)\n",
    "            else:\n",
    "                gen_step_features[\"correct_attachment_point_choice\"] = []\n",
    "            gen_step_features[\"valid_attachment_point_choices\"] = gen_step.valid_attachment_point_choices\n",
    "\n",
    "            # And finally, the correct node type choices. Here, we have an empty list of\n",
    "            # correct choices for all steps where we didn't choose a node, so we skip that:\n",
    "            if gen_step.correct_node_type_choices is not None:\n",
    "                gen_step_features[\"correct_node_type_choices\"] = self.node_types_to_multi_hot(gen_step.correct_node_type_choices)\n",
    "            else:\n",
    "                gen_step_features[\"correct_node_type_choices\"] = []\n",
    "            gen_step_features['correct_first_node_type_choices'] = self.node_types_to_multi_hot(molecule.correct_first_node_type_choices)\n",
    "            \n",
    "            # Add graph_property_values\n",
    "            gen_step_features = {**gen_step_features, **molecule_property_values}\n",
    "\n",
    "        molecule_gen_steps = self._to_tensor_moler(molecule_gen_steps)\n",
    "        return [Data(**step) for step in molecule_gen_steps]\n",
    "    \n",
    "    def _to_tensor_moler(self, molecule_gen_steps):\n",
    "        for i in range(len(molecule_gen_steps)):\n",
    "            for k,v in molecule_gen_steps[i].items():\n",
    "                molecule_gen_steps[i][k] = torch.tensor(molecule_gen_steps[i][k])\n",
    "        return molecule_gen_steps\n",
    "    \n",
    "    def len(self):\n",
    "        return self.processed_file_names_size\n",
    "\n",
    "    def get(self, idx):\n",
    "        file_path = self.processed_file_names[idx]\n",
    "        data = torch.load(file_path)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ece8ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MolerDataset(\n",
    "    root = '/data/ongh0068', \n",
    "    raw_moler_trace_dataset_parent_folder = '/data/ongh0068/l1000/trace_playground',\n",
    "    output_pyg_trace_dataset_parent_folder = '/data/ongh0068/l1000/pyg_output_playground',\n",
    "    split = 'train',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df1f3f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MolerDataset(2384)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f20fae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=False, follow_batch = [\n",
    "    'correct_edge_choices',\n",
    "    'correct_edge_types',\n",
    "    'valid_edge_choices',\n",
    "    'valid_attachment_point_choices',\n",
    "    'correct_attachment_point_choice',\n",
    "    'correct_node_type_choices'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e604534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([126, 32])\n",
      "edge_index: torch.Size([2, 212])\n",
      "original_graph_x: torch.Size([464, 32])\n",
      "original_graph_edge_index: torch.Size([2, 1024])\n",
      "original_graph_edge_type: torch.Size([1024])\n",
      "original_graph_node_categorical_features: torch.Size([464])\n",
      "focus_node: torch.Size([16])\n",
      "edge_type: torch.Size([212])\n",
      "edge_features: torch.Size([53, 3])\n",
      "correct_edge_choices: torch.Size([53])\n",
      "correct_edge_choices_batch: torch.Size([53])\n",
      "correct_edge_choices_ptr: torch.Size([17])\n",
      "num_correct_edge_choices: torch.Size([16])\n",
      "stop_node_label: torch.Size([16])\n",
      "valid_edge_choices: torch.Size([53, 2])\n",
      "valid_edge_choices_batch: torch.Size([53])\n",
      "valid_edge_choices_ptr: torch.Size([17])\n",
      "correct_edge_types: torch.Size([9, 3])\n",
      "correct_edge_types_batch: torch.Size([9])\n",
      "correct_edge_types_ptr: torch.Size([17])\n",
      "partial_node_categorical_features: torch.Size([126])\n",
      "correct_attachment_point_choice: torch.Size([0])\n",
      "correct_attachment_point_choice_batch: torch.Size([0])\n",
      "correct_attachment_point_choice_ptr: torch.Size([17])\n",
      "valid_attachment_point_choices: torch.Size([0])\n",
      "valid_attachment_point_choices_batch: torch.Size([0])\n",
      "valid_attachment_point_choices_ptr: torch.Size([17])\n",
      "correct_node_type_choices: torch.Size([1112])\n",
      "correct_node_type_choices_batch: torch.Size([1112])\n",
      "correct_node_type_choices_ptr: torch.Size([17])\n",
      "correct_first_node_type_choices: torch.Size([2224])\n",
      "sa_score: torch.Size([16])\n",
      "clogp: torch.Size([16])\n",
      "mol_weight: torch.Size([16])\n",
      "qed: torch.Size([16])\n",
      "bertz: torch.Size([16])\n",
      "batch: torch.Size([126])\n",
      "ptr: torch.Size([17])\n"
     ]
    }
   ],
   "source": [
    "def pprint_pyg_obj(batch):\n",
    "    for key in vars(batch)['_store'].keys():\n",
    "        if key.startswith('_'):\n",
    "            continue\n",
    "        print(f'{key}: {batch[key].shape}')\n",
    "for batch in loader:\n",
    "    pprint_pyg_obj(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77d5b454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some information out from the dataset:\n",
    "next_node_type_distribution = dataset.metadata.get(\"train_next_node_type_distribution\")\n",
    "atom_type_nums = [\n",
    "    next_node_type_distribution[dataset.node_type_index_to_string[type_idx]]\n",
    "    for type_idx in range(dataset.num_node_types)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8cce787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 16,  16,  16,  16,  16,  16,  16,  16, 129,  16,  16,  16,  16, 129,\n",
       "         16,  16,  16,  16, 129, 129,  16,  16,  16,  16, 129, 129,  16,  16,\n",
       "         16,  16, 129, 129, 129,  16,  16,  16,  16, 129, 129, 129,  16,  16,\n",
       "         16,  16, 129, 129, 129, 129,  16,  16,  16,  16, 129, 129, 129, 129,\n",
       "         16,  16,  16,  16, 129, 129, 129, 129, 129,  16,  16,  16,  16, 129,\n",
       "        129, 129, 129, 129,  16,  16,  16,  16, 129, 129, 129, 129, 129, 129,\n",
       "         16,  16,  16,  16, 129, 129, 129, 129, 129, 129,  16,  16,  16,  16,\n",
       "        129, 129, 129, 129, 129, 129,  16,  16,  16,  16, 129, 129, 129, 129,\n",
       "        129, 129, 131,  16,  16,  16,  16, 129, 129, 129, 129, 129, 129, 131])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.partial_node_categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994b7ed",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "Depending on the representation of the molecular graph (whether bond type is represented as edge type or as edge attributes), we have to change the function signature of the forward method\n",
    "\n",
    "1. Try out both\n",
    "2. tune number of heads\n",
    "3. Look into softmax aggregation (hyperparams) + also implement sigmoidaggregation as a separate aggregation layer\n",
    "4. Set number of heads as a hyperparam\n",
    "5. Add leakyrelu() + layer norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a0ab358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([126, 32])\n",
      "edge_index: torch.Size([2, 212])\n",
      "original_graph_x: torch.Size([464, 32])\n",
      "original_graph_edge_index: torch.Size([2, 1024])\n",
      "original_graph_edge_type: torch.Size([1024])\n",
      "original_graph_node_categorical_features: torch.Size([464])\n",
      "focus_node: torch.Size([16])\n",
      "edge_type: torch.Size([212])\n",
      "edge_features: torch.Size([53, 3])\n",
      "correct_edge_choices: torch.Size([53])\n",
      "correct_edge_choices_batch: torch.Size([53])\n",
      "correct_edge_choices_ptr: torch.Size([17])\n",
      "num_correct_edge_choices: torch.Size([16])\n",
      "stop_node_label: torch.Size([16])\n",
      "valid_edge_choices: torch.Size([53, 2])\n",
      "valid_edge_choices_batch: torch.Size([53])\n",
      "valid_edge_choices_ptr: torch.Size([17])\n",
      "correct_edge_types: torch.Size([9, 3])\n",
      "correct_edge_types_batch: torch.Size([9])\n",
      "correct_edge_types_ptr: torch.Size([17])\n",
      "partial_node_categorical_features: torch.Size([126])\n",
      "correct_attachment_point_choice: torch.Size([0])\n",
      "correct_attachment_point_choice_batch: torch.Size([0])\n",
      "correct_attachment_point_choice_ptr: torch.Size([17])\n",
      "valid_attachment_point_choices: torch.Size([0])\n",
      "valid_attachment_point_choices_batch: torch.Size([0])\n",
      "valid_attachment_point_choices_ptr: torch.Size([17])\n",
      "correct_node_type_choices: torch.Size([1112])\n",
      "correct_node_type_choices_batch: torch.Size([1112])\n",
      "correct_node_type_choices_ptr: torch.Size([17])\n",
      "correct_first_node_type_choices: torch.Size([2224])\n",
      "sa_score: torch.Size([16])\n",
      "clogp: torch.Size([16])\n",
      "mol_weight: torch.Size([16])\n",
      "qed: torch.Size([16])\n",
      "bertz: torch.Size([16])\n",
      "batch: torch.Size([126])\n",
      "ptr: torch.Size([17])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(dataset.node_type_index_to_string)\n",
    "embedding_size = 64\n",
    "for batch in loader:\n",
    "    pprint_pyg_obj(batch)\n",
    "    break\n",
    "    \n",
    "embed = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "motif_embeddings = embed(batch.partial_node_categorical_features)\n",
    "batch.x = torch.cat((batch.x, motif_embeddings), axis = -1)\n",
    "\n",
    "from torch_geometric.nn import RGATConv\n",
    "resize_layer = RGATConv(\n",
    "in_channels = batch.x.shape[-1], \n",
    "out_channels = 64, \n",
    "num_relations = 3\n",
    ")\n",
    "\n",
    "\n",
    "batch.x = resize_layer(batch.x, batch.edge_index.long(), batch.edge_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46bab97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import RGATConv\n",
    "from torch_geometric.nn import aggr\n",
    "class GraphEncoder(torch.nn.Module):\n",
    "    \"\"\"For constructing graph level embedding during encoder step\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_feature_dim,\n",
    "        node_feature_dim = 64,\n",
    "        num_layers = 12,\n",
    "        use_intermediate_gnn_results = True,\n",
    "    ):\n",
    "        super(GraphEncoder, self).__init__()\n",
    "        self._first_layer = RGATConv(\n",
    "            in_channels = input_feature_dim, \n",
    "            out_channels = node_feature_dim, \n",
    "            num_relations = 3\n",
    "        )\n",
    "        \n",
    "        self._encoder_layers = torch.nn.ModuleList(\n",
    "            [RGATConv(\n",
    "                in_channels = node_feature_dim, \n",
    "                out_channels = node_feature_dim, \n",
    "                num_relations = 3\n",
    "                ) for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self._softmax_aggr = aggr.SoftmaxAggregation(learn=True)\n",
    "        self._use_intermediate_gnn_results = use_intermediate_gnn_results\n",
    "    def forward(self, node_features, edge_index, edge_type, batch_index):\n",
    "        gnn_results = []\n",
    "        gnn_results += [self._first_layer(node_features, edge_index.long(), edge_type)]\n",
    "        print(gnn_results[-1].shape)\n",
    "        for i, layer in enumerate(self._encoder_layers):\n",
    "            gnn_results += [layer(gnn_results[-1], edge_index.long(), edge_type)]\n",
    "        \n",
    "        if self._use_intermediate_gnn_results:\n",
    "            x = torch.cat(gnn_results, axis = -1)\n",
    "            graph_representations = self._softmax_aggr(x, batch_index)\n",
    "        \n",
    "        else:\n",
    "            graph_representations = self._softmax_aggr(gnn_results[-1], batch_index)\n",
    "        node_representations = torch.cat(gnn_results, axis= -1)\n",
    "        return graph_representations, node_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80204491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([126, 64])\n"
     ]
    }
   ],
   "source": [
    "model = GraphEncoder(input_feature_dim = batch.x.shape[-1])\n",
    "input_molecule_representations, _ = model(batch.x, batch.edge_index.long(), batch.edge_type, batch.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "817a46f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 832])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_molecule_representations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05bcced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4114d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanAndLogVarMLP(torch.nn.Module):\n",
    "    \"\"\"For constructing graph level embedding during encoder step\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dim = 768,\n",
    "        latent_dim = 512,\n",
    "        num_layers = 1,\n",
    "    ):\n",
    "        super(MeanAndLogVarMLP, self).__init__()\n",
    "        self._mean_log_var_mlp = torch.nn.Linear(input_dim, 2*latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._mean_log_var_mlp(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46d4f575",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_log_var_mlp = MeanAndLogVarMLP(input_dim = input_molecule_representations.shape[-1])\n",
    "x = mean_log_var_mlp(input_molecule_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc95dbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1024])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape\n",
    "\n",
    "# this represents the aggregated graph representations after passing through \n",
    "# the mean and log var mlp => we need to sample from this distribution\n",
    "# for each of the partial graphs. There are 16 partial graphs, so \n",
    "# we need to get a separate sample for each of them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a28a1d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = x[:, : latent_dim]  # Shape: [V, MD]\n",
    "log_var = x[:, latent_dim :]  # Shape: [V, MD]\n",
    "\n",
    "# result_representations: shape [num_partial_graphs, latent_repr_dim]\n",
    "std = torch.exp(log_var / 2)\n",
    "p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "q = torch.distributions.Normal(mu, std)\n",
    "z = q.rsample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24046627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b2dac7",
   "metadata": {},
   "source": [
    "# Decoder\n",
    " Every MLP should condition on the latent representation (currently computed from the partial graph itself which is wrong, should be from the original full molecular graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83885293",
   "metadata": {},
   "source": [
    "## PickAtomOrMotif + Node type selection loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14578290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([126, 64])\n"
     ]
    }
   ],
   "source": [
    "decoder_gnn = GraphEncoder(input_feature_dim = batch.x.shape[-1])\n",
    "partial_graph_representions, node_representations = model(batch.x, batch.edge_index.long(), batch.edge_type, batch.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97aa07fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 832])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_graph_representions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccbb28c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.x.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad9dba9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([126, 64])\n",
      "edge_index: torch.Size([2, 212])\n",
      "original_graph_x: torch.Size([464, 32])\n",
      "original_graph_edge_index: torch.Size([2, 1024])\n",
      "original_graph_edge_type: torch.Size([1024])\n",
      "original_graph_node_categorical_features: torch.Size([464])\n",
      "focus_node: torch.Size([16])\n",
      "edge_type: torch.Size([212])\n",
      "edge_features: torch.Size([53, 3])\n",
      "correct_edge_choices: torch.Size([53])\n",
      "correct_edge_choices_batch: torch.Size([53])\n",
      "correct_edge_choices_ptr: torch.Size([17])\n",
      "num_correct_edge_choices: torch.Size([16])\n",
      "stop_node_label: torch.Size([16])\n",
      "valid_edge_choices: torch.Size([53, 2])\n",
      "valid_edge_choices_batch: torch.Size([53])\n",
      "valid_edge_choices_ptr: torch.Size([17])\n",
      "correct_edge_types: torch.Size([9, 3])\n",
      "correct_edge_types_batch: torch.Size([9])\n",
      "correct_edge_types_ptr: torch.Size([17])\n",
      "partial_node_categorical_features: torch.Size([126])\n",
      "correct_attachment_point_choice: torch.Size([0])\n",
      "correct_attachment_point_choice_batch: torch.Size([0])\n",
      "correct_attachment_point_choice_ptr: torch.Size([17])\n",
      "valid_attachment_point_choices: torch.Size([0])\n",
      "valid_attachment_point_choices_batch: torch.Size([0])\n",
      "valid_attachment_point_choices_ptr: torch.Size([17])\n",
      "correct_node_type_choices: torch.Size([1112])\n",
      "correct_node_type_choices_batch: torch.Size([1112])\n",
      "correct_node_type_choices_ptr: torch.Size([17])\n",
      "correct_first_node_type_choices: torch.Size([2224])\n",
      "sa_score: torch.Size([16])\n",
      "clogp: torch.Size([16])\n",
      "mol_weight: torch.Size([16])\n",
      "qed: torch.Size([16])\n",
      "bertz: torch.Size([16])\n",
      "batch: torch.Size([126])\n",
      "ptr: torch.Size([17])\n"
     ]
    }
   ],
   "source": [
    "pprint_pyg_obj(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ffcc22b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  2,  4,  6,  8, 10, 13, 15])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.correct_node_type_choices_batch.unique() # graphs requiring node choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1b31a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,  139,  139,  278,  278,  417,  417,  556,  556,  695,  695,  834,\n",
       "         834,  834,  973,  973, 1112])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.correct_node_type_choices_ptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17110c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_requiring_node_choices = batch.correct_node_type_choices_batch.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "079c242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx_graphs_requiring_node_choices = batch.correct_node_type_choices_ptr[graphs_requiring_node_choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07e89aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0, 139, 278, 417, 556, 695, 834, 973])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_idx_graphs_requiring_node_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60cc903a",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type_multihot_labels = []\n",
    "for i in range(len(batch.correct_node_type_choices_ptr)-1):\n",
    "    start_idx = batch.correct_node_type_choices_ptr[i]\n",
    "    end_idx = batch.correct_node_type_choices_ptr[i+1] \n",
    "    if end_idx - start_idx == 0:\n",
    "        continue\n",
    "    node_selection_labels = batch.correct_node_type_choices[start_idx: end_idx]\n",
    "    node_type_multihot_labels += [node_selection_labels]\n",
    "    \n",
    "node_type_multihot_labels = torch.stack(node_type_multihot_labels, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae6c0758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_type_multihot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42e9b378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "class PickAtomOrMotifMLP(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    For choosing an atom/motif out of the motif vocabulary\n",
    "    Notes:\n",
    "    Softmax layer at the end with \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, num_node_types, latent_vector_dim, hidden_channels=64):\n",
    "        super(PickAtomOrMotifMLP, self).__init__()\n",
    "        self.hidden_layer1 = Linear(latent_vector_dim, hidden_channels)\n",
    "        self.hidden_layer2 = Linear(hidden_channels, num_node_types + 1) # add 1 for <END OF GENERATION TOKEN>\n",
    "\n",
    "        \n",
    "    def forward(self, original_and_calculated_graph_representations):\n",
    "        print(original_and_calculated_graph_representations.shape)\n",
    "        x = self.hidden_layer1(original_and_calculated_graph_representations)\n",
    "        x = self.hidden_layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c88ca5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type_selector = PickAtomOrMotifMLP(\n",
    "    num_node_types = dataset.num_node_types,\n",
    "    latent_vector_dim = z.shape[-1] + partial_graph_representions.shape[-1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e0e3464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1344])\n",
      "torch.Size([8, 1344])\n"
     ]
    }
   ],
   "source": [
    "# result_representations: shape [PG, MD]\n",
    "# number of partial graphs, molecule representation dimension\n",
    "\n",
    "def pick_node_type(\n",
    "    input_molecule_representations,\n",
    "    graph_representations,\n",
    "    graphs_requiring_node_choices\n",
    "):\n",
    "    \n",
    "    relevant_graph_representations = input_molecule_representations[graphs_requiring_node_choices]\n",
    "    relevant_input_molecule_representations = graph_representations[graphs_requiring_node_choices]\n",
    "    original_and_calculated_graph_representations = torch.cat((relevant_graph_representations,relevant_input_molecule_representations), axis = -1)\n",
    "    print(original_and_calculated_graph_representations.shape)\n",
    "    return node_type_selector(original_and_calculated_graph_representations)\n",
    "\n",
    "output = pick_node_type(z, partial_graph_representions, graphs_requiring_node_choices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3b9d35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_divide_loss(loss, num_choices):\n",
    "    \"\"\"Divide `loss` by `num_choices`, but guard against `num_choices` being 0.\"\"\"\n",
    "    return loss / max(num_choices, 1.0) \n",
    "SMALL_NUMBER, BIG_NUMBER = 1e-7, 1e7\n",
    "def compute_neglogprob_for_multihot_objective(\n",
    "    logprobs,\n",
    "    multihot_labels,\n",
    "    per_decision_num_correct_choices,\n",
    "):\n",
    "    # Normalise by number of correct choices and mask out entries for wrong decisions:\n",
    "    return -(\n",
    "        (logprobs + torch.log(per_decision_num_correct_choices + SMALL_NUMBER))\n",
    "        * multihot_labels\n",
    "        / (per_decision_num_correct_choices + SMALL_NUMBER)\n",
    "    )\n",
    "\n",
    "def compute_node_type_selection_loss(\n",
    "    node_type_logits,\n",
    "    node_type_multihot_labels\n",
    "):\n",
    "    per_node_decision_logprobs = torch.nn.functional.log_softmax(node_type_logits, dim = -1)\n",
    "    # Shape: [NTP, NT + 1]\n",
    "    \n",
    "    # number of correct choices for each of the partial graphs that require node choices\n",
    "    per_node_decision_num_correct_choices = torch.sum(node_type_multihot_labels, keepdim = True, axis = -1)\n",
    "    # Shape [NTP, 1]\n",
    "    \n",
    "    per_correct_node_decision_normalised_neglogprob = compute_neglogprob_for_multihot_objective(\n",
    "        logprobs = per_node_decision_logprobs[:, :-1], # separate out the no node prediction\n",
    "        multihot_labels = node_type_multihot_labels,\n",
    "        per_decision_num_correct_choices = per_node_decision_num_correct_choices,\n",
    "    ) # Shape [NTP, NT]\n",
    "    \n",
    "    no_node_decision_correct =(per_node_decision_num_correct_choices == 0.0).sum()  # Shape [NTP]\n",
    "    per_correct_no_node_decision_neglogprob = -(\n",
    "        per_node_decision_logprobs[:, -1]\n",
    "        * torch.squeeze(no_node_decision_correct).type(torch.FloatTensor)\n",
    "    )  # Shape [NTP]\n",
    "\n",
    "#     if self._node_type_loss_weights is not None:\n",
    "#         per_correct_node_decision_normalised_neglogprob *= self._node_type_loss_weights[:-1]\n",
    "#         per_correct_no_node_decision_neglogprob *= self._node_type_loss_weights[-1]\n",
    "    \n",
    "    # Loss is the sum of the masked (no) node decisions, averaged over number of decisions made:\n",
    "    total_node_type_loss = torch.sum(\n",
    "        per_correct_node_decision_normalised_neglogprob\n",
    "    ) + torch.sum(per_correct_no_node_decision_neglogprob)\n",
    "    node_type_loss = safe_divide_loss(\n",
    "        total_node_type_loss, node_type_multihot_labels.shape[0]\n",
    "    )\n",
    "\n",
    "    return node_type_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14fe295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type_selection_loss = compute_node_type_selection_loss(\n",
    "    node_type_logits = output,\n",
    "    node_type_multihot_labels = node_type_multihot_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "528977a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type_selection_loss.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df66735",
   "metadata": {},
   "source": [
    "## Pick edge and edge type + Edge candidate loss\n",
    "\n",
    "1. Find out what stop node refers to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6dc9673d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2135581705.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [38], line 13\u001b[0;36m\u001b[0m\n\u001b[0;31m    # logits for the type of edge if it is picked\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def pick_edge(\n",
    "    input_molecule_representations,\n",
    "    graph_representations,\n",
    "    node_representations,\n",
    "    graph_to_focus_node_map,\n",
    "    node_to_graph_map,\n",
    "    candidate_edge_targets,\n",
    "    candidate_edge_features,\n",
    "    graphs_requiring_node_choices\n",
    "):\n",
    "    # given candidate edges in the partial graphs, \n",
    "    #compute logits for the likelihood of adding candidates as well as \n",
    "    # logits for the type of edge if it is picked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b2c8fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([126, 64])\n",
      "edge_index: torch.Size([2, 212])\n",
      "original_graph_x: torch.Size([464, 32])\n",
      "original_graph_edge_index: torch.Size([2, 1024])\n",
      "original_graph_edge_type: torch.Size([1024])\n",
      "original_graph_node_categorical_features: torch.Size([464])\n",
      "focus_node: torch.Size([16])\n",
      "edge_type: torch.Size([212])\n",
      "edge_features: torch.Size([53, 3])\n",
      "correct_edge_choices: torch.Size([53])\n",
      "correct_edge_choices_batch: torch.Size([53])\n",
      "correct_edge_choices_ptr: torch.Size([17])\n",
      "num_correct_edge_choices: torch.Size([16])\n",
      "stop_node_label: torch.Size([16])\n",
      "valid_edge_choices: torch.Size([53, 2])\n",
      "valid_edge_choices_batch: torch.Size([53])\n",
      "valid_edge_choices_ptr: torch.Size([17])\n",
      "correct_edge_types: torch.Size([9, 3])\n",
      "correct_edge_types_batch: torch.Size([9])\n",
      "correct_edge_types_ptr: torch.Size([17])\n",
      "partial_node_categorical_features: torch.Size([126])\n",
      "correct_attachment_point_choice: torch.Size([0])\n",
      "correct_attachment_point_choice_batch: torch.Size([0])\n",
      "correct_attachment_point_choice_ptr: torch.Size([17])\n",
      "valid_attachment_point_choices: torch.Size([0])\n",
      "valid_attachment_point_choices_batch: torch.Size([0])\n",
      "valid_attachment_point_choices_ptr: torch.Size([17])\n",
      "correct_node_type_choices: torch.Size([1112])\n",
      "correct_node_type_choices_batch: torch.Size([1112])\n",
      "correct_node_type_choices_ptr: torch.Size([17])\n",
      "correct_first_node_type_choices: torch.Size([2224])\n",
      "sa_score: torch.Size([16])\n",
      "clogp: torch.Size([16])\n",
      "mol_weight: torch.Size([16])\n",
      "qed: torch.Size([16])\n",
      "bertz: torch.Size([16])\n",
      "batch: torch.Size([126])\n",
      "ptr: torch.Size([17])\n"
     ]
    }
   ],
   "source": [
    "pprint_pyg_obj(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ba67ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_node_idx_in_batch = batch.ptr[:-1] + batch.focus_node # offset each focus node idx by the starting idx of the molecule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "add41206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 832])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_representations[focus_node_idx_in_batch].shape # extract focus node node level repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0ff72e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_node_representations = node_representations[focus_node_idx_in_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d3b88500",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_and_focus_node_representations = torch.cat(\n",
    "    (input_molecule_representations, partial_graph_representions, focus_node_representations),\n",
    "    axis = -1\n",
    ")\n",
    "\n",
    "# Explanation: at each step, there is a focus node, which is the node we are \n",
    "# focusing on right now in terms of adding another edge to it. When adding a new\n",
    "# edge, the edge can be between the focus node and a variety of other nodes.\n",
    "# This is likely based on valency, and in reality, it is possible that none of the\n",
    "# edge choices are correct (when that generation step is a node addition step)\n",
    "# and not an edge addition step. Regardless, we still want to consider the candidates\n",
    "#\"target\" refers to the node at the other end of the candidate edge\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19bc3384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 0, 0, 4, 0, 0, 4, 5, 0, 4, 0, 4, 5, 6, 0, 4, 5, 0, 4, 5, 6, 7, 0,\n",
       "        4, 5, 6, 0, 4, 5, 6, 7, 8, 0, 5, 6, 7, 8, 0, 5, 6, 7, 0, 5, 6, 7, 8, 9,\n",
       "        0, 5, 6, 7, 9], dtype=torch.int32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.valid_edge_choices[:, 1] # the 2nd element in each edge is the target since\n",
    "# the 1st element is the focus node at that step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "74749c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_edge_target_node_idx = batch.valid_edge_choices[:, 1] + batch.ptr[batch.valid_edge_choices_batch]  # idx offset for the edge choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0286be3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_graph_idx_per_edge_candidate = batch.batch[candidate_edge_target_node_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ac022b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_and_focus_node_representations = torch.cat(\n",
    "    (input_molecule_representations, partial_graph_representions, focus_node_representations),\n",
    "    axis = -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0fdca4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_and_focus_node_representations_per_edge_candidate = graph_and_focus_node_representations[partial_graph_idx_per_edge_candidate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2b9b85b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_candidate_target_node_representations = node_representations[candidate_edge_target_node_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e85d8e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_edge_features = batch.edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "738f3c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 3., 0., 0., 4., 0., 0., 0., 5., 2., 0., 0., 0., 0., 6., 3., 2.,\n",
       "        0., 0., 0., 0., 0., 7., 4., 3., 2., 0., 0., 0., 0., 0., 0., 4., 2., 3.,\n",
       "        4., 5., 4., 2., 3., 2., 0., 0., 0., 0., 0., 0., 6., 4., 3., 2., 2.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_truncation = 10\n",
    "candidate_edge_features[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eaa47148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The zeroth element of edge_features is the graph distance. We need to look that up\n",
    "# in the distance embeddings:\n",
    "truncated_distances = torch.minimum(\n",
    "    candidate_edge_features[:, 0],\n",
    "    torch.ones(len(candidate_edge_features)) * (distance_truncation - 1),\n",
    ")  # shape: [CE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c07b171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_distances = truncated_distances.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b38b31a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we want to truncate the distance, we should have an embedding layer for it\n",
    "distance_embedding_layer = torch.nn.Embedding(distance_truncation, 1)\n",
    "\n",
    "distance_embedding = distance_embedding_layer(truncated_distances)\n",
    "\n",
    "edge_candidate_representation = torch.cat(\n",
    "    (\n",
    "        graph_and_focus_node_representations_per_edge_candidate, \n",
    "        edge_candidate_target_node_representations,\n",
    "        distance_embedding,\n",
    "        candidate_edge_features[:, 1:],\n",
    "    ),\n",
    "    axis = -1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dbaf3f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([53, 3331])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_candidate_representation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "956873ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_and_focus_node_representations.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0605bede",
   "metadata": {},
   "outputs": [],
   "source": [
    "_no_more_edges_representation = torch.nn.Parameter(torch.FloatTensor(1,node_representations.shape[-1] + batch.edge_features.shape[-1]), requires_grad = True)\n",
    "# Calculate the stop node features as well.\n",
    "num_graphs_in_batch= graph_and_focus_node_representations.shape[0]\n",
    "stop_edge_selection_representation = torch.cat(\n",
    "    [\n",
    "        graph_and_focus_node_representations,\n",
    "        torch.tile(\n",
    "            _no_more_edges_representation,\n",
    "            dims=(num_graphs_in_batch, 1),\n",
    "        ),\n",
    "    ],\n",
    "    axis=-1,\n",
    ")  # shape: [PG, MD + PD + 2 * VD*(num_layers+1) + FD]\n",
    "\n",
    "edge_candidate_and_stop_features = torch.cat(\n",
    "    [edge_candidate_representation, stop_edge_selection_representation], axis=0\n",
    ")  # shape: [CE + PG, MD + PD + 2 * VD*(num_layers+1) + FD]\n",
    "\n",
    "\n",
    "from torch.nn import Linear\n",
    "class EdgeCandidateScorer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    For choosing an atom/motif out of the motif vocabulary\n",
    "    Notes:\n",
    "    Softmax layer at the end with \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, latent_vector_dim, output_size = 1, hidden_channels=64):\n",
    "        super(EdgeCandidateScorer, self).__init__()\n",
    "        self.hidden_layer1 = Linear(latent_vector_dim, hidden_channels)\n",
    "        self.hidden_layer2 = Linear(hidden_channels, output_size) # add 1 for <END OF GENERATION TOKEN>\n",
    "\n",
    "        \n",
    "    def forward(self, original_and_calculated_graph_representations):\n",
    "        x = self.hidden_layer1(original_and_calculated_graph_representations)\n",
    "        x = self.hidden_layer2(x)\n",
    "        return x\n",
    "\n",
    "class EdgeTypeSelector(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    For choosing an atom/motif out of the motif vocabulary\n",
    "    Notes:\n",
    "    Softmax layer at the end with \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, latent_vector_dim,num_edge_types = 3, hidden_channels=64):\n",
    "        super(EdgeTypeSelector, self).__init__()\n",
    "        self.hidden_layer1 = Linear(latent_vector_dim, hidden_channels)\n",
    "        self.hidden_layer2 = Linear(hidden_channels, num_edge_types) # add 1 for <END OF GENERATION TOKEN>\n",
    "\n",
    "        \n",
    "    def forward(self, original_and_calculated_graph_representations):\n",
    "        x = self.hidden_layer1(original_and_calculated_graph_representations)\n",
    "        x = self.hidden_layer2(x)\n",
    "        return x\n",
    "\n",
    "_edge_candidate_scorer = EdgeCandidateScorer(latent_vector_dim = edge_candidate_and_stop_features.shape[-1])\n",
    "_edge_type_selector = EdgeTypeSelector(latent_vector_dim = edge_candidate_and_stop_features.shape[-1])\n",
    "edge_candidate_logits = torch.squeeze(\n",
    "    _edge_candidate_scorer(edge_candidate_and_stop_features),\n",
    "    axis=-1,\n",
    ")  # shape: [CE + PG]\n",
    "edge_type_logits = _edge_type_selector(\n",
    "    edge_candidate_representation\n",
    ")  # shape: [CE, ET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fe2a4e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([69]), torch.Size([53, 3]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_candidate_logits.shape, edge_type_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7e6d8aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([53])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.correct_edge_choices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "74f11841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.correct_edge_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "374b295c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  1,  2,  3,  3,  4,  5,  5,  5,  6,  6,  7,  7,  7,  7,  8,  8,  8,\n",
       "         9,  9,  9,  9,  9, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 12, 12, 12,\n",
       "        12, 12, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.correct_edge_choices_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "acfb88d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_candidate_to_graph_map = batch.batch[candidate_edge_target_node_idx]\n",
    "# add the end bond labels to the end \n",
    "edge_candidate_to_graph_map = torch.cat((edge_candidate_to_graph_map, torch.arange(0, num_graphs_in_batch)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dc157ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import scatter \n",
    "def traced_unsorted_segment_log_softmax(\n",
    "    logits, #edge_candidate_logits\n",
    "    segment_ids, #edge_candidate_to_graph_map\n",
    "    num_segments, # num_graphs_in_batch\n",
    "):\n",
    "    \n",
    "    max_per_segment = scatter(logits, segment_ids, reduce = 'max')\n",
    "    scattered_maxes = max_per_segment[segment_ids]\n",
    "    recentered_scores = logits - scattered_maxes\n",
    "    exped_recentered_scores = torch.exp(recentered_scores)\n",
    "\n",
    "    per_segment_sums = scatter(exped_recentered_scores, segment_ids, reduce = 'sum')\n",
    "    per_segment_normalization_consts = torch.log(per_segment_sums)\n",
    "\n",
    "    log_probs = recentered_scores - per_segment_normalization_consts[segment_ids]\n",
    "    return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "23c13928",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_candidate_logprobs = traced_unsorted_segment_log_softmax(\n",
    "    logits = edge_candidate_logits,\n",
    "    segment_ids = edge_candidate_to_graph_map,\n",
    "    num_segments = num_graphs_in_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1c95a70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_edge_candidate_selection_loss(\n",
    "    num_graphs_in_batch, # correct_node_type_choices\n",
    "    node_to_graph_map, #batch.batch\n",
    "    candidate_edge_targets, # batch_features[\"valid_edge_choices\"][:, 1]\n",
    "    edge_candidate_logits, # as is\n",
    "    per_graph_num_correct_edge_choices, # batch.num_correct_edge_choices\n",
    "    edge_candidate_correctness_labels, # correct edge choices\n",
    "    no_edge_selected_labels # stop node label\n",
    "):\n",
    "    \n",
    "    # First, we construct full labels for all edge decisions, which are the concat of\n",
    "    # edge candidate logits and the logits for choosing no edge:\n",
    "    edge_correctness_labels = torch.cat(\n",
    "        [edge_candidate_correctness_labels, no_edge_selected_labels.float()],\n",
    "        axis=0,\n",
    "    )  # Shape: [CE + PG]\n",
    "\n",
    "    # To compute a softmax over all candidate edges (and the \"no edge\" choice) corresponding\n",
    "    # to the same graph, we first need to build the map from each logit to the corresponding\n",
    "    # graph id. Then, we can do an unsorted_segment_softmax using that map:\n",
    "    edge_candidate_to_graph_map = batch.batch[candidate_edge_target_node_idx]\n",
    "    # add the end bond labels to the end \n",
    "    edge_candidate_to_graph_map = torch.cat((edge_candidate_to_graph_map, torch.arange(0, num_graphs_in_batch)))\n",
    "\n",
    "    edge_candidate_logprobs = traced_unsorted_segment_log_softmax(\n",
    "        logits=edge_candidate_logits,\n",
    "        segment_ids=edge_candidate_to_graph_map,\n",
    "        num_segments=num_graphs_in_batch,\n",
    "    )  # Shape: [CE + PG]\n",
    "\n",
    "    # Compute the edge loss with the multihot objective.\n",
    "    # For a single graph with three valid choices (+ stop node) of which two are correct,\n",
    "    # we may have the following:\n",
    "    #  edge_candidate_logprobs = log([0.05, 0.5, 0.4, 0.05])\n",
    "    #  per_graph_num_correct_edge_choices = [2]\n",
    "    #  edge_candidate_correctness_labels = [0.0, 1.0, 1.0]\n",
    "    #  edge_correctness_labels = [0.0, 1.0, 1.0, 0.0]\n",
    "    # To get the loss, we simply look at the things in edge_candidate_logprobs that correspond\n",
    "    # to correct entries.\n",
    "    # However, to account for the _multi_hot nature, we scale up each entry of\n",
    "    # edge_candidate_logprobs by the number of correct choices, i.e., consider the\n",
    "    # correct entries of\n",
    "    #  log([0.05, 0.5, 0.4, 0.05]) + log([2, 2, 2, 2]) = log([0.1, 1.0, 0.8, 0.1])\n",
    "    # In this form, we want to have each correct entry to be as near possible to 1.\n",
    "    # Finally, we normalise loss contributions to by-graph, by dividing the crossentropy\n",
    "    # loss by the number of correct choices (i.e., in the example above, this results in\n",
    "    # a loss of -((log(1.0) + log(0.8)) / 2) = 0.11...).\n",
    "\n",
    "    # Note: per_graph_num_correct_edge_choices does not include the choice of an edge to\n",
    "    # the stop node, so can be zero.\n",
    "    per_graph_num_correct_edge_choices = torch.max(\n",
    "        per_graph_num_correct_edge_choices, torch.ones(per_graph_num_correct_edge_choices.shape)\n",
    "    )  # Shape: [PG]\n",
    "\n",
    "\n",
    "    per_edge_candidate_num_correct_choices = per_graph_num_correct_edge_choices[edge_candidate_to_graph_map]\n",
    "    # Shape: [CE]\n",
    "    per_correct_edge_neglogprob = -(\n",
    "        (edge_candidate_logprobs + torch.log(per_edge_candidate_num_correct_choices))\n",
    "        * edge_correctness_labels\n",
    "        / per_edge_candidate_num_correct_choices\n",
    "    )  # Shape: [CE]\n",
    "    \n",
    "    # Normalise by number of graphs for which we made edge selection decisions:\n",
    "    edge_loss = safe_divide_loss(\n",
    "        torch.sum(per_correct_edge_neglogprob), num_graphs_in_batch\n",
    "    )\n",
    "    print(edge_correctness_labels, per_edge_candidate_num_correct_choices)\n",
    "\n",
    "    return edge_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "de820bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1.]) tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2., 2., 2., 2., 2., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.0640e+21, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "compute_edge_candidate_selection_loss(\n",
    "    num_graphs_in_batch = len(batch.ptr) -1, # correct_node_type_choices\n",
    "    node_to_graph_map = batch.batch, #batch.batch\n",
    "    candidate_edge_targets = batch.valid_edge_choices[:, 1], # batch_features[\"valid_edge_choices\"][:, 1]\n",
    "    edge_candidate_logits = edge_candidate_logits, # as is\n",
    "    per_graph_num_correct_edge_choices = batch.num_correct_edge_choices, # batch.num_correct_edge_choices\n",
    "    edge_candidate_correctness_labels = batch.correct_edge_choices, # correct edge choices\n",
    "    no_edge_selected_labels = batch.stop_node_label.float() # stop node label\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6a4dea31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1.])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb0b33e2",
   "metadata": {},
   "source": [
    "    edge_loss = self.compute_edge_candidate_selection_loss(\n",
    "        num_graphs_in_batch=batch_features[\"num_partial_graphs_in_batch\"],\n",
    "        node_to_graph_map=batch_features[\"node_to_partial_graph_map\"],\n",
    "        candidate_edge_targets=batch_features[\"valid_edge_choices\"][:, 1],\n",
    "        edge_candidate_logits=task_output.edge_candidate_logits,\n",
    "        per_graph_num_correct_edge_choices=batch_labels[\"num_correct_edge_choices\"],\n",
    "        edge_candidate_correctness_labels=batch_labels[\"correct_edge_choices\"],\n",
    "        no_edge_selected_labels=batch_labels[\"stop_node_label\"],\n",
    "    )\n",
    "\n",
    "Shape abbreviations used throughout the model:\n",
    "- PG = number of _p_artial _g_raphs\n",
    "- PV = number of _p_artial graph _v_ertices\n",
    "- PD = size of _p_artial graph _representation _d_imension\n",
    "- VD = GNN _v_ertex representation _d_imension\n",
    "- MD = _m_olecule representation _d_imension\n",
    "- EFD = _e_dge _f_eature _d_imension\n",
    "- NTP = number of partial graphs requiring a _n_ode _t_ype _p_ick\n",
    "- NT = number of _n_ode _t_ypes\n",
    "- CE = number of _c_andidate _e_dges\n",
    "- CCE = number of _c_orrect _c_andidate _e_dges\n",
    "- ET = number of _e_dge _t_ypes\n",
    "- CA = number of _c_andidate _a_ttachment points\n",
    "- AP = number of _a_ttachment _p_oint choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce99eabe",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "1. Implement Sampling strategy\n",
    "2. Look into why first node prediction is treated separately\n",
    "3. Add dropout layers\n",
    "\n",
    "Current design plan: all model specific things are in the BaseModel class, \n",
    "All loss computation and what not will be in the lightning module \n",
    "This decouples the training code, optimizers and schedulers from the model itself, which arguably makes it cleaner when doing hyperparameter tuning and also during model loading\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Note: Original implementation of the GNN adds each molecule and their corresponding generation steps sequentially into the batch, so what happens is that a molecule's sequential generation steps are seen by the GNN during training in a sequential manner. In this implementation, if we use shuffle = False, we can replicate this behaviour in our dataloader, but if we use shuffle = True, then we can't. \n",
    "\n",
    "Possible additions to allow for molecule wise shuffling => add additional column to CSV to allow for a molecule's generation steps to be treated as one set of training samples => then shuffle this instead of the actual generation steps (try this if the training is not converging)\n",
    "\n",
    "This has additional implications for the latent space sampling strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a88b540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class MoLeROutput:\n",
    "    \"\"\"Class for keeping track of output from MoLeR model.\"\"\"\n",
    "    node_type_logits\n",
    "    edge_candidate_logits\n",
    "    edge_type_logits\n",
    "    attachment_point_selection_logits\n",
    "    p\n",
    "    q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e010f0",
   "metadata": {},
   "source": [
    "# Follow implementation here: https://github.com/Lightning-AI/lightning-bolts/blob/master/pl_bolts/models/autoencoders/basic_vae/basic_vae_module.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438dc5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from molecule_generation.utils.training_utils import get_class_balancing_weights\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "\n",
    "\n",
    "\n",
    "class BaseModel(LightningModule):\n",
    "    def __init__(self, params, dataset):\n",
    "        \"\"\"Params is a nested dictionary with the relevant parameters.\"\"\"\n",
    "        super(BaseModel, self).__init__()\n",
    "        self._init_params(params, dataset)\n",
    "        self._motif_aware_embedding_layer = Embedding(params['motif_aware_embedding'])\n",
    "        self._encoder_gnn = FullGraphEncoder(params['encoder_gnn'])\n",
    "        self._decoder_gnn = PartialGraphEncoder(params['decoder_gnn'])\n",
    "        self._decoder_mlp = MLPDecoder(params['decoder_mlp'])\n",
    "        \n",
    "        # params for latent space\n",
    "        self._latent_sample_strategy = params['latent_sample_strategy']\n",
    "        self._latent_repr_dim = params[\"latent_repr_size\"]\n",
    "        \n",
    "        \n",
    "               \n",
    "    def _init_params(self, params, dataset):\n",
    "        \"\"\"\n",
    "        Initialise class weights for next node prediction and placefolder for\n",
    "        motif/node embeddings.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get some information out from the dataset:\n",
    "        next_node_type_distribution = dataset.metadata.get(\"train_next_node_type_distribution\")\n",
    "        class_weight_factor = self._params.get(\"node_type_predictor_class_loss_weight_factor\", 0.0)\n",
    "        \n",
    "        if not (0.0 <= class_weight_factor <= 1.0):\n",
    "            raise ValueError(\n",
    "                f\"Node class loss weight node_classifier_class_loss_weight_factor must be in [0,1], but is {class_weight_factor}!\"\n",
    "            )\n",
    "        if class_weight_factor > 0:\n",
    "            atom_type_nums = [\n",
    "                next_node_type_distribution[dataset.node_type_index_to_string[type_idx]]\n",
    "                for type_idx in range(dataset.num_node_types)\n",
    "            ]\n",
    "            atom_type_nums.append(next_node_type_distribution[\"None\"])\n",
    "\n",
    "            self.class_weights = get_class_balancing_weights(\n",
    "                class_counts=atom_type_nums, class_weight_factor=class_weight_factor\n",
    "            )\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "            \n",
    "        motif_vocabulary = dataset.metadata.get(\"motif_vocabulary\")\n",
    "        self._uses_motifs = motif_vocabulary is not None\n",
    "\n",
    "        self._node_categorical_num_classes = dataset.node_categorical_num_classes\n",
    "        \n",
    "        \n",
    "        if self.uses_categorical_features:\n",
    "            if \"categorical_features_embedding_dim\" in self._params:\n",
    "                self._node_categorical_features_embedding = None\n",
    "        \n",
    "    @property\n",
    "    def uses_motifs(self):\n",
    "        return self._uses_motifs\n",
    "\n",
    "    @property\n",
    "    def uses_categorical_features(self):\n",
    "        return self._node_categorical_num_classes is not None\n",
    "\n",
    "    @property\n",
    "    def decoder(self):\n",
    "        return self._decoder\n",
    "\n",
    "    @property\n",
    "    def encoder(self):\n",
    "        return self._encoder\n",
    "    \n",
    "    @property\n",
    "    def motif_aware_embedding_layer(self):\n",
    "        return self._motif_aware_embedding_layer\n",
    "    \n",
    "    @property\n",
    "    def latent_dim(self):\n",
    "        return self._latent_repr_dim\n",
    "    \n",
    "    def compute_initial_node_features(batch )\n",
    "        # Compute embedding\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    def sample_from_latent_repr(latent_repr):\n",
    "        # perturb latent repr\n",
    "        mu = latent_repr[:, : self.latent_dim]  # Shape: [V, MD]\n",
    "        log_var = latent_repr[:, self.latent_dim :]  # Shape: [V, MD]\n",
    "\n",
    "        # result_representations: shape [num_partial_graphs, latent_repr_dim]\n",
    "        p, q, z = self.sample(mu, log_var)\n",
    "        \n",
    "        return p, q, z \n",
    "        \n",
    "    def sample(self, mu, log_var)\n",
    "        \"\"\"Samples a different noise vector for each partial graph. \n",
    "        TODO: look into the other sampling strategies.\"\"\"\n",
    "        std = torch.exp(log_var / 2)\n",
    "        p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "        z = q.rsample()\n",
    "        return p, q, z\n",
    "    \n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr, batch, ??):\n",
    "        # Obtain node embeddings \n",
    "        batch = self.compute_initial_node_features(batch)\n",
    "        \n",
    "        # Forward pass through encoder\n",
    "        latent_repr = self.encoder(batch)\n",
    "        \n",
    "        # Apply latent sampling strategy\n",
    "        p, q, latent_repr = self.sample_from_latent_repr(latent_repr)\n",
    "        \n",
    "        # Forward pass through decoder\n",
    "        node_type_logits, edge_candidate_logits, edge_type_logits, attachment_point_selection_logits = self.decoder(latent_repr)\n",
    "        \n",
    "        # NOTE: loss computation will be done in lightning module\n",
    "        return MoLeROutput(\n",
    "            node_type_logits = node_type_logits,\n",
    "            edge_candidate_logits = edge_candidate_logits,\n",
    "            edge_type_logits = edge_type_logits,\n",
    "            attachment_point_selection_logits = attachment_point_selection_logits,\n",
    "            p = p,\n",
    "            q = q,\n",
    "        )\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "#         x = self.conv1(x, edge_index, edge_attr)\n",
    "#         x = x.relu()\n",
    "#         x = self.conv2(x, edge_index, edge_attr)\n",
    "#         x = x.relu()\n",
    "#         x = self.conv3(x, edge_index, edge_attr)\n",
    "#         return x\n",
    "#     def _compute_decoder_loss_and_metrics(\n",
    "#         self, batch_features, task_output, batch_labels\n",
    "#     ) -> Tuple[tf.Tensor, MoLeRDecoderMetrics]:\n",
    "\n",
    "#         decoder_metrics = self.decoder.compute_metrics(\n",
    "#             batch_features=batch_features, batch_labels=batch_labels, task_output=task_output\n",
    "#         )\n",
    "\n",
    "#         total_loss = (\n",
    "#             self._params[\"node_classification_loss_weight\"]\n",
    "#             * decoder_metrics.node_classification_loss\n",
    "#             + self._params[\"first_node_classification_loss_weight\"]\n",
    "#             * decoder_metrics.first_node_classification_loss\n",
    "#             + self._params[\"edge_selection_loss_weight\"] * decoder_metrics.edge_loss\n",
    "#             + self._params[\"edge_type_loss_weight\"] * decoder_metrics.edge_type_loss\n",
    "#         )\n",
    "\n",
    "#         if self.uses_motifs:\n",
    "#             total_loss += (\n",
    "#                 self._params[\"attachment_point_selection_weight\"]\n",
    "#                 * decoder_metrics.attachment_point_selection_loss\n",
    "#             )\n",
    "\n",
    "#         return total_loss, decoder_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dd147f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c4c98cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphEncoder(torch.nn.Module):\n",
    "    \"\"\"For constructing graph level embedding during encoder step\"\"\"\n",
    "    def __init__(self, num_node_features, hidden_channels):\n",
    "        super(GraphEncoder, self).__init__()\n",
    "        # torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = x.leaky_relu()\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = x.leaky_relu()\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = \n",
    "        return x\n",
    "\n",
    "\n",
    "class PartialGraphEncoder(torch.nn.Module):\n",
    "    \"\"\"For constructing graph level embedding during decoder steps\"\"\"\n",
    "    def __init__(self, num_node_features, hidden_channels):\n",
    "        super(PartialGraphEncoder, self).__init__()\n",
    "        # torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        return x\n",
    "  \n",
    "\n",
    "class PickAtomOrMotifMLP(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    For choosing an atom/motif out of the motif vocabulary\n",
    "    Notes:\n",
    "    Softmax layer at the end with \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, motif_vocabulary, latent_vector_dim, hidden_channels=256):\n",
    "        super(PickAtomOrMotifMLP, self).__init__()\n",
    "        self.hidden_layer1 = Linear(latent_vector_dim, hidden_channels)\n",
    "        self.hidden_layer2 = Linear(hidden_channels, len(motif_vocabulary) + 1) # add 1 for <END OF GENERATION TOKEN>\n",
    "        self.activation = torch.Softmax\n",
    "        \n",
    "    def forward(self, latent_vector, partial_graph_embedding):\n",
    "        x = self.hidden_layer1(latent_vector)\n",
    "        x = self.hidden_layer2(x)\n",
    "        return self.activation(x)\n",
    "        \n",
    "\n",
    "class PickAttachmentMLP(torch.nn.Module):\n",
    "    def __init__(self, motif_dictionary, hidden_channels):\n",
    "        super(PickAttachmentMLP, self).__init__()\n",
    "        pass\n",
    "    def forward(self, latent_vector, partial_graph_embedding):\n",
    "        pass\n",
    "\n",
    "class PickBond(torch.nn.Module):\n",
    "    def __init__(self, motif_dictionary, hidden_channels):\n",
    "        super(PickAttachmentMLP, self).__init__()\n",
    "        pass\n",
    "    def forward(self, latent_vector, partial_graph_embedding):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b776f10a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe5585c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f849029d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30276ddd",
   "metadata": {},
   "source": [
    "# TODOs\n",
    "\n",
    "1. Implement the learned aggregation function in Moler Paper by subclassing MessagePassing layer \n",
    "2. Change the init function of the encoder to take in the relevant hyperparameters from the `moler_vae` file\n",
    "3. Figure out which of the latent sample strategies work best:\n",
    "- pass through\n",
    "- per graph\n",
    "- per partial graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f28cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
