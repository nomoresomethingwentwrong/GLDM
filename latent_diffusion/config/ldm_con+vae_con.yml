model:
  base_learning_rate: 1.0e-03
  target: ldm.models.diffusion.ddpm.LatentDiffusion
  params:
    linear_start: 0.00085
    linear_end: 0.012
    num_timesteps_cond: 1
    log_every_t: 200
    timesteps: 1000
    first_stage_key: image
    cond_stage_key: gene_expressions
    image_size: 512
    channels: 1
    cond_stage_trainable: False
    conditioning_key: crossattn
    monitor: val/loss_simple_ema
    scale_factor: 1 #0.18215
    use_ema: False
    parameterization: eps

    scheduler_config: # 10000 warmup steps
      target: ldm.lr_scheduler.LambdaLinearScheduler
      params:
        warm_up_steps: [10000]
        cycle_lengths: [10000000000000]
        f_start: [1.e-6]
        f_max: [1.]
        f_min: [ 1.]

  first_stage_config:
    target: model.BaseModel
    model_type: vae
    using_lincs: True
    ckpt_path: /data/conghao001/FYP/DrugDiscovery/first_stage_models/2023-03-11_23_33_36.921147/epoch=07-val_loss=0.60.ckpt
  
  cond_stage_config:
    params: 
      dim: 979
      key: gene_expressions

  unet_config:
    target: ldm.modules.diffusionmodules.openaimodel.UNetModel
    params:
      use_spatial_transformer: true
      image_size: 512
      in_channels: 1
      out_channels: 1
      model_channels: 64
      dims: 1 
      attention_resolutions:
      # note: this isn\t actually the resolution but
      # the downsampling factor, i.e. this corresnponds to
      # attention on spatial resolution 8,16,32, as the
      # spatial reolution of the latents is 64 for f4
      - 4
      - 2
      num_res_blocks: 1
      channel_mult:
      - 1
      - 2
      - 3
      context_dim: 979
      num_head_channels: 8